{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn Imports\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring the warnings, pretending everything is fine *whistle*\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training data\n",
    "dataset_origin = pd.read_csv(\"Data/train.csv\")\n",
    "dataset_origin.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the original dataframe to keep a copy untouched\n",
    "dataset = dataset_origin.copy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by using the dataframe with a version in which we dealt with the \"NaN\" values.\n",
    "\n",
    "After that, we will see how our previous analysis can be used to adapt and improve our data selection and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe cleaning up that we did in the previous notebook\n",
    "\n",
    "# Step 1 : setting a new index\n",
    "dataset.set_index(\"Id\", inplace=True)\n",
    "\n",
    "# Step 2 : replacing \"NaN\" values with \"NA\" as it was intended.\n",
    "dataset_nan_values = [\n",
    "    \"Alley\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageType\",\n",
    "    \"GarageFinish\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "    \"Fence\",\n",
    "    \"MiscFeature\",\n",
    "]\n",
    "\n",
    "for columns in dataset_nan_values:\n",
    "    dataset[columns].fillna(\"NA\", inplace=True)\n",
    "\n",
    "# Step 3 : replacing \"NaN\" values in LotFrontage column with most frequent data\n",
    "dataset[\"LotFrontage\"] = dataset[\"LotFrontage\"].fillna(\n",
    "    dataset[\"LotFrontage\"].value_counts().index[0]\n",
    ")\n",
    "\n",
    "# Step 4 : replacing \"NaN\" values in MasVnrType column with most frequent data\n",
    "dataset[\"MasVnrType\"] = dataset[\"MasVnrType\"].fillna(\n",
    "    dataset[\"MasVnrType\"].value_counts().index[0]\n",
    ")\n",
    "\n",
    "# Step 5 : replacing \"NaN\" values in MasVnrArea column with most frequent data\n",
    "dataset[\"MasVnrArea\"] = dataset[\"MasVnrArea\"].fillna(\n",
    "    dataset[\"MasVnrArea\"].value_counts().index[0]\n",
    ")\n",
    "\n",
    "# Step 6 : replacing \"NaN\" values in Electrical column with most frequent data\n",
    "dataset[\"Electrical\"].fillna(dataset[\"Electrical\"].mode()[0], inplace=True)\n",
    "\n",
    "# Step 7 : replacing \"NaN\" values in \"GarageYrBlt\" column with \"YearBuilt\" data\n",
    "dataset[\"GarageYrBlt\"].fillna(dataset[\"YearBuilt\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Non-Null Count Verification: \\n {dataset.info()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe is a bit cleaned up, we can start trying out different algorithms using regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Prediction\n",
    "\n",
    "### Regression Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the \"big_fit\" function that was used in Titanic and we'll adapt it to the regression algorithms that we will try out first.\n",
    "\n",
    "I've selected :\n",
    "- Gradient Boosting Regressor\n",
    "- Random Forest Regressor\n",
    "- Decision Tree Regressor\n",
    "\n",
    "Others could be tested but you've got to start somewhere, right?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Fit Function for our algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train different algorithms with the following parameters\n",
    "def big_fit(X_train, y_train, X_test, y_test, transformer):\n",
    "\n",
    "    # Regression models that we will try out\n",
    "    estimators = [\n",
    "        (\"gbr\", GradientBoostingRegressor(random_state=42)),\n",
    "        (\"rfr\", RandomForestRegressor(random_state=42)),\n",
    "        (\"dtr\", DecisionTreeRegressor(random_state=42)),\n",
    "    ]\n",
    "\n",
    "    # Setting up a dictionnary to store algorithm scores\n",
    "    default_error = dict()\n",
    "\n",
    "    # Scoring parameters\n",
    "    for estimator in estimators:\n",
    "        pipe = Pipeline(steps=[(\"transformer\", transformer), estimator])\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        mea_score = mean_absolute_error(y_test, y_pred)\n",
    "        mse_score = mean_squared_error(y_test, y_pred)\n",
    "        medae_score = median_absolute_error(y_test, y_pred)\n",
    "        r2score = r2_score(y_test, y_pred)\n",
    "        name = estimator[0]\n",
    "        default_error[name] = {\n",
    "            \"mea\": mea_score,\n",
    "            \"mse\": mse_score,\n",
    "            \"medae\": medae_score,\n",
    "            \"r2\": r2score,\n",
    "        }\n",
    "\n",
    "    # Hyperparamaters for each algorithm\n",
    "    param_grids = [\n",
    "        {\n",
    "            \"gbr__n_estimators\": [100, 250, 500],\n",
    "            \"gbr__min_samples_split\": np.arange(2, 5),\n",
    "            \"gbr__min_samples_leaf\": np.arange(1, 4),\n",
    "            \"gbr__max_features\": [\"sqrt\", \"log2\", 0.2, 0.3, 0.4],\n",
    "        },\n",
    "        {\n",
    "            \"rfr__n_estimators\": [100, 250, 500],\n",
    "            \"rfr__min_samples_split\": np.arange(2, 5),\n",
    "            \"rfr__min_samples_leaf\": np.arange(1, 4),\n",
    "            \"rfr__max_features\": [\"sqrt\", \"log2\", 0.2, 0.3, 0.4],\n",
    "        },\n",
    "        {\n",
    "            \"dtr__min_samples_split\": np.arange(2, 5),\n",
    "            \"dtr__min_samples_leaf\": np.arange(1, 4),\n",
    "            \"dtr__max_features\": [\"sqrt\", \"log2\", 0.2, 0.3, 0.4],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Dictionnaries to store training scores\n",
    "    best_estimators = dict()\n",
    "    best_params = dict()\n",
    "    best_scores = dict()\n",
    "    predict_error = dict()\n",
    "    error = dict()\n",
    "\n",
    "    # Gridsearch\n",
    "    score = \"r2\"\n",
    "\n",
    "    for estimator, param_grid in zip(estimators, param_grids):\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            Pipeline(steps=[(\"preprocessor\", preprocessor), estimator]),\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring=score,\n",
    "            return_train_score=True,\n",
    "            verbose=0,\n",
    "            n_jobs=2,\n",
    "        )\n",
    "\n",
    "        name = estimator[0]\n",
    "        print(f\"Trainning with {name}\")\n",
    "\n",
    "        # Fit train\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        # Bests\n",
    "        best_estimators[name] = grid.best_estimator_\n",
    "        best_params[name] = grid.best_params_\n",
    "        best_scores[name] = round(grid.best_score_ * 100, 2)\n",
    "\n",
    "        # Prediction\n",
    "        y_pred = grid.predict(X_test)\n",
    "\n",
    "        # Scores\n",
    "        mea_score = mean_absolute_error(y_test, y_pred)\n",
    "        mse_score = mean_squared_error(y_test, y_pred)\n",
    "        medae_score = median_absolute_error(y_test, y_pred)\n",
    "        r2score = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Predict Error & Errors\n",
    "        predict_error[name] = {\n",
    "            \"mea\": mea_score,\n",
    "            \"mse\": mse_score,\n",
    "            \"medae\": medae_score,\n",
    "            \"r2\": r2score,\n",
    "        }\n",
    "        error[name] = [abs(x - y) for x, y in zip(y_test, y_pred)]\n",
    "\n",
    "    return (\n",
    "        default_error,\n",
    "        best_scores,\n",
    "        error,\n",
    "        predict_error,\n",
    "        best_params,\n",
    "        best_estimators,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Big fit alternative to keep in mind\n",
    "\n",
    "def big_fit(X_train, y_train, X_test, y_test):\n",
    "    estimators = [\n",
    "        (\"rfr\", RandomForestRegressor(random_state=42)),\n",
    "        (\"gbr\", GradientBoostingRegressor(random_state=42)),\n",
    "    ]\n",
    "\n",
    "    default_scores = dict()\n",
    "\n",
    "    for estimator in estimators:\n",
    "        model = estimator[1]\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        scores = [\n",
    "            mean_squared_error(y_test, y_pred),\n",
    "            median_absolute_error(y_test, y_pred),\n",
    "            mean_absolute_error(y_test, y_pred),\n",
    "            r2_score(y_test, y_pred),\n",
    "        ]\n",
    "        name = estimator[0]\n",
    "\n",
    "        print(f\"Trainning with {name}\")\n",
    "        default_scores[name] = [round(s, 2) for s in scores]\n",
    "\n",
    "    param_grids = [\n",
    "        {\n",
    "            \"n_estimators\": np.arange(100, 400, 100),\n",
    "        },\n",
    "        {\n",
    "            \"n_estimators\": np.arange(100, 400, 100),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    best_estimators = dict()\n",
    "    best_params = dict()\n",
    "    best_scores = dict()\n",
    "    predict_scores = dict()\n",
    "\n",
    "    for estimator, param_grid in zip(estimators, param_grids):\n",
    "        grid = GridSearchCV(\n",
    "            estimator[1],\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            return_train_score=True,\n",
    "            verbose=1,\n",
    "            n_jobs=8,\n",
    "        )\n",
    "        name = estimator[0]\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_estimators[name] = grid.best_estimator_\n",
    "        best_params[name] = grid.best_params_\n",
    "        best_scores[name] = round(grid.best_score_, 2)\n",
    "\n",
    "        y_pred = grid.predict(X_test)\n",
    "        scores = [\n",
    "            mean_squared_error(y_test, y_pred),\n",
    "            median_absolute_error(y_test, y_pred),\n",
    "            mean_absolute_error(y_test, y_pred),\n",
    "            r2_score(y_test, y_pred),\n",
    "        ]\n",
    "        predict_scores[name] = [round(s, 2) for s in scores]\n",
    "\n",
    "    return default_scores, best_scores, predict_scores, best_params, best_estimators\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Learning Curves Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting the learning curves\n",
    "\n",
    "\n",
    "def learning_curve(name, model, X_train, y_train):\n",
    "\n",
    "    # Using the default scoring method \"r2\" then we'll see\n",
    "    score = \"r2\"\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        train_sizes=np.linspace(0.1, 1, 20),\n",
    "        cv=5,\n",
    "        random_state=42,\n",
    "        scoring=score,\n",
    "    )\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    # Plot details\n",
    "    plt.title(f\"Learning Curve with {name}\")\n",
    "    plt.xlabel(\"Train size\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross validation score\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Error Histograms Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting error histograms (or Errorists)\n",
    "\n",
    "\n",
    "def error_hist(name, model):\n",
    "    plt.title(f\"Error Distribution for {name}\")\n",
    "    plt.hist(model, bins=50)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting numerical values and categorical values to transform them differently\n",
    "numerical_features = make_column_selector(dtype_include=np.number)\n",
    "categorical_features = make_column_selector(dtype_exclude=np.number)\n",
    "\n",
    "# Using RobustScaler + KNNImputer on numerical values to better deal with outliers\n",
    "numerical_pipeline = make_pipeline(KNNImputer(), RobustScaler())\n",
    "\n",
    "# Using SimpleImputer and OneHotEncoder on categorical values\n",
    "categorical_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"),\n",
    ")\n",
    "\n",
    "# Putting the two pipelines together\n",
    "preprocessor = make_column_transformer(\n",
    "    (numerical_pipeline, numerical_features),\n",
    "    (categorical_pipeline, categorical_features),\n",
    ")\n",
    "\n",
    "# Splitting features from target\n",
    "features, target = dataset.drop(\"SalePrice\", axis=1), dataset[\"SalePrice\"]\n",
    "\n",
    "# Splitting the dataframe in two to have one part for training and one part for testing\n",
    "feat_train, feat_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Let's PAR-TAY with your Big Fit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing alternative to keep in mind\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    # Ici Ã©ventuellement vos Imputers...\n",
    "    (RobustScaler(), numeric_cols), # Ou autre\n",
    "    (OneHotEncoder(drop=\"first\"), categorical_cols),  # Ou autre (C'est ce que fait pd.get_dummies())\n",
    ")\n",
    "\n",
    "df_train_X, df_train_y = df_train.drop(\"SalePrice\", axis=1), df_train[\"SalePrice\"]\n",
    "\n",
    "df_train_X_enc = transformer.fit_transform(df_train_X)\n",
    "\n",
    "trans_features = transformer.get_feature_names_out()\n",
    "print(f\"{trans_features.size} features after trans\")\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(df_train_new_X_enc, df_train_new_y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Let's BIG FIT\")\n",
    "\n",
    "default_scores, best_scores, predict_scores, best_params, best_estimators = big_fit(\n",
    "    X_train_new, y_train_new, X_test_new, y_test_new\n",
    ")\n",
    "print(\"Train Finished! \\o/\")\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the algorithms with our transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    default_scores,\n",
    "    best_scores,\n",
    "    predict_scores,\n",
    "    best_params,\n",
    "    best_estimators,\n",
    "    mae,\n",
    "    rmse,\n",
    "    meae,\n",
    "    err,\n",
    "    feat_names,\n",
    "    feat_scores,\n",
    ") = big_fit(feat_train, target_train, feat_test, target_test, preprocessor)\n",
    "\n",
    "print(\"Train Finished! Let's pop the champagne!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe to store the scores and better visualise them\n",
    "algorithm_scores = pd.DataFrame(\n",
    "    {\n",
    "        \"Name\": [name for name in best_estimators.keys()],\n",
    "        \"Default Test Score\": list(default_scores.values()),\n",
    "        \"Grid Cross-Validation Score\": list(best_scores.values()),\n",
    "        \"Grid Test Score\": list(predict_scores.values()),\n",
    "        \"Mean Abs Error\": list(mae.values()),\n",
    "        \"RMSE\": list(rmse.values()),\n",
    "        \"Median Abs Error\": list(meae.values()),\n",
    "    }\n",
    ")\n",
    "\n",
    "algorithm_scores.sort_values(by=\"Grid Test Score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which algorithm performed the best\n",
    "best_algo = max(predict_scores, key=predict_scores.get)\n",
    "print(f\"The algorithm that performed the best is : {best_algo}\")\n",
    "winners = {\"default\": best_estimators[best_algo]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which hyperparameters where the best\n",
    "for name, it in best_params.items():\n",
    "    print(f\"{name}: \", end=\"\")\n",
    "    print(\" ; \".join(f\"{p.split('__')[1]}: {pv}\" for p, pv in it.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(best_algo, best_estimators[best_algo], feat_train, target_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I do not discriminate against low performing algorithms and because they tried their best, I'm still going to look at their learning curves.\n",
    "\n",
    "That's how I roll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve(\"gbr\", best_estimators[\"gbr\"], feat_train, target_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit ('HousePrices')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e6ba7d0fce325db5fe7fb8689ecf9566225a59436b96c86c517e78a89dc1ffe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
